# ğŸš€ Walk-Forward Hybrid LSTM + XGBoost Model Training

This directory contains a comprehensive machine learning pipeline for training cryptocurrency trading models using a hybrid approach that combines LSTM (Long Short-Term Memory) neural networks with XGBoost gradient boosting, implemented with **walk-forward validation** for realistic out-of-sample performance estimation.

## ğŸ—ï¸ Architecture Overview

### ğŸ§  Two-Model Hybrid System

```
ğŸ“Š Raw OHLCV Data
       â†“
ğŸ”§ Feature Engineering (Technical Indicators)
       â†“
ğŸ§  LSTM Model â†’ lstm_delta (price change prediction)
       â†“
ğŸŒ² XGBoost Model (lstm_delta + features) â†’ Final Trade Signal
```

### ğŸ¯ Model Responsibilities

| Component | Input | Output | Purpose |
|-----------|-------|--------|---------|
| **LSTM** | 60 candles of [price, volume] | `lstm_delta` (% price change) | Learn temporal patterns |
| **XGBoost** | Technical indicators + `lstm_delta` | Buy/Sell signal (binary) | Final trading decision |

## ğŸ“ Project Structure

```
trade_bot_2.0/
â”œâ”€â”€ ğŸ“Š data/                          # SQLite databases (from binance_data_collector.py)
â”‚   â”œâ”€â”€ btceur_15m.db
â”‚   â”œâ”€â”€ etheur_15m.db
â”‚   â”œâ”€â”€ adaeur_15m.db
â”‚   â”œâ”€â”€ soleur_15m.db
â”‚   â””â”€â”€ xrpeur_15m.db
â”œâ”€â”€ ğŸ¤– models/                        # Trained models (created by training)
â”‚   â”œâ”€â”€ lstm/                         # LSTM models (.h5 files)
â”‚   â”œâ”€â”€ xgboost/                      # XGBoost models (.pkl files)
â”‚   â””â”€â”€ scalers/                      # Data scalers (.pkl files)
â”œâ”€â”€ ğŸ“Š results/                       # Training results and analysis
â”‚   â”œâ”€â”€ training_summary.json
â”‚   â”œâ”€â”€ {symbol}_evaluation.json
â”‚   â””â”€â”€ {symbol}_feature_importance.csv
â”œâ”€â”€ ğŸ”§ train_hybrid_models.py         # Main training script
â”œâ”€â”€ ğŸ“‹ requirements_ml.txt            # ML dependencies
â””â”€â”€ ğŸ“– README_TRAINING.md             # This file
```

## ğŸ› ï¸ Installation

### 1. Install Dependencies

```bash
# Install ML requirements
pip install -r requirements_ml.txt

# Note: This project uses pandas-ta for technical analysis,
# which is easier to install than TA-Lib and provides the same functionality.
```

### 2. Ensure Data is Available

Make sure you've run the data collector first:

```bash
python binance_data_collector.py
```

## ğŸš€ Usage

### Quick Start

```bash
# Train models for all cryptocurrency pairs
python train_hybrid_models.py

# Use custom data and model directories with warm start
python train_hybrid_models.py --data-dir ./data --models-dir ./models --warm-start
```

### Command Line Options

- `--data-dir`: location of the SQLite files
- `--models-dir`: where trained models are saved
- `--warm-start`: load the previous window's model before training

### Walk-Forward Training Process

The script implements **walk-forward validation** with a sliding window approach:

#### ğŸ”„ Walk-Forward Configuration
- **Training Window**: 6 months of data
- **Test Window**: 1 month of data  
- **Step Size**: 1 month (monthly retraining)
- **Minimum Samples**: 10,000 for training, 1,000 for LSTM sequences

#### ğŸ“Š Step 1: Data Preparation
- Loads 15-minute OHLCV data from SQLite databases
- Creates 30+ technical indicators:
  - **Price**: Returns, price changes (1h, 4h)
  - **Volume**: Volume ratios, changes
  - **Volatility**: ATR, rolling standard deviation
  - **Moving Averages**: EMA(9,21,50), SMA(200)
  - **Oscillators**: RSI, MACD, Bollinger Bands
  - **Market Structure**: VWAP, support/resistance levels
  - **Time Features**: Hour, day of week, weekend indicator
- Generates walk-forward time windows

#### ğŸ”„ Step 2: Walk-Forward Loop
For each time window:

##### ğŸ§  LSTM Training
- **Input**: 60-step sequences of [price, volume] from training window
- **Architecture**: `LSTM(64) â†’ Dropout(0.2) â†’ LSTM(32) â†’ Dense(32) â†’ Dense(1)`
- **Target**: Next 15-minute price change percentage
- **Split**: 80% train, 20% validation within window
- **Output**: `lstm_delta` predictions for both training and test periods

##### ğŸŒ² XGBoost Training
- **Input**: Technical indicators + `lstm_delta` from training window
- **Target**: Binary classification (price up > 0.5%)
- **Features**: 25+ engineered features including LSTM output
- **Split**: 80% train, 20% validation within window
- **Optimization**: Reduced estimators for faster training

##### ğŸ“Š Window Evaluation
- **LSTM Metrics**: MAE, RMSE on test window
- **XGBoost Metrics**: Accuracy, AUC on test window
- **Aggregation**: Results collected across all windows

#### ğŸ“ˆ Step 3: Final Model Persistence
- Saves models from the **last window** (most recent)
- Generates feature importance from final XGBoost model
- Creates comprehensive walk-forward performance reports

## ğŸ“Š Expected Output

### Console Output
```
================================================================================
ğŸš€ WALK-FORWARD HYBRID LSTM + XGBOOST TRAINING PIPELINE
================================================================================
ğŸ“Š Symbols: BTCEUR, ETHEUR, ADAEUR, SOLEUR, XRPEUR
ğŸ“… Walk-Forward Config: 6M train â†’ 1M test (step: 1M)
â° Started at: 2025-01-11 15:30:45

============================================================
ğŸš€ Walk-Forward Training for BTCEUR
============================================================

ğŸ“Š Data Preparation
ğŸ“Š Loaded 95,847 candles for BTCEUR
ğŸ“… Date range: 2020-01-01 to 2025-01-11
âœ… Created 32 technical features
ğŸ”„ Generated 54 walk-forward windows

ğŸ”„ Window 1/54: 2020-01-01 to 2020-08-01
ğŸ§  LSTM Training: 17,280 sequences
ğŸŒ² XGBoost Training: 17,219 samples
ğŸ“Š Window 1 Results:
   LSTM: MAE=0.008234, RMSE=0.015678
   XGBoost: Accuracy=0.6789, AUC=0.7456

ğŸ”„ Window 2/54: 2020-02-01 to 2020-09-01
...

ğŸ”„ Window 54/54: 2024-07-01 to 2025-01-01
ğŸ“Š Window 54 Results:
   LSTM: MAE=0.007891, RMSE=0.014523
   XGBoost: Accuracy=0.7123, AUC=0.7834

âœ… Final models saved for BTCEUR
ğŸ” Top 5 features: ['lstm_delta', 'rsi', 'macd_histogram', 'price_vs_ema9', 'volume_ratio']

ğŸ“Š BTCEUR Summary (54 windows):
   LSTM: MAE=0.008012Â±0.000456, RMSE=0.015234Â±0.000789
   XGBoost: Accuracy=0.6934Â±0.0234, AUC=0.7612Â±0.0187
â±ï¸  Completed in 23.4 minutes

================================================================================
ğŸ‰ WALK-FORWARD TRAINING COMPLETED!
================================================================================
â° Total time: 187.3 minutes
âœ… Successful: 5/5 symbols
ğŸ“Š Total windows processed: 270

ğŸ“ˆ Overall Performance:
   LSTM: MAE=0.008156Â±0.000523
   XGBoost: Accuracy=0.6891Â±0.0198, AUC=0.7534Â±0.0156

ğŸ“ Results saved to: results/walkforward_results_20250111_183045.json
```

### Generated Files

#### Models Directory
```
models/
â”œâ”€â”€ lstm/
â”‚   â”œâ”€â”€ btceur_lstm.h5              # Trained LSTM model
â”‚   â”œâ”€â”€ btceur_history.pkl          # Training history
â”‚   â””â”€â”€ ... (other symbols)
â”œâ”€â”€ xgboost/
â”‚   â”œâ”€â”€ btceur_xgboost.pkl          # Trained XGBoost model
â”‚   â””â”€â”€ ... (other symbols)
â””â”€â”€ scalers/
    â”œâ”€â”€ btceur_scaler.pkl           # Data scaler for LSTM
    â””â”€â”€ ... (other symbols)
```

#### Results Directory
```
results/
â”œâ”€â”€ walkforward_results_20250111_183045.json  # Comprehensive walk-forward results
â”œâ”€â”€ btceur_feature_importance.csv             # Feature rankings from final model
â”œâ”€â”€ etheur_feature_importance.csv             # Feature rankings from final model
â””â”€â”€ ... (other symbols)
```

#### Walk-Forward Results Structure
```json
{
  "timestamp": "20250111_183045",
  "training_type": "walk_forward",
  "config": {
    "train_months": 6,
    "test_months": 1,
    "step_months": 1,
    "min_training_samples": 10000
  },
  "total_time_minutes": 187.3,
  "symbols_trained": 5,
  "total_windows": 270,
  "aggregated_metrics": {
    "avg_lstm_mae": 0.008156,
    "avg_lstm_rmse": 0.015234,
    "avg_xgb_accuracy": 0.6891,
    "avg_xgb_auc": 0.7534,
    "std_lstm_mae": 0.000523,
    "std_xgb_accuracy": 0.0198
  },
  "results_by_symbol": {
    "BTCEUR": [
      {
        "window": 1,
        "train_start": "2020-01-01",
        "train_end": "2020-07-01",
        "test_end": "2020-08-01",
        "lstm_mae": 0.008234,
        "lstm_rmse": 0.015678,
        "xgb_accuracy": 0.6789,
        "xgb_auc": 0.7456
      },
      // ... more windows
    ]
  }
}
```

## ğŸ”§ Configuration

### Key Parameters (in `train_hybrid_models.py`)

```python
# Walk-Forward Configuration
train_months = 6                    # 6 months training window
test_months = 1                     # 1 month test window
step_months = 1                     # 1 month step size (monthly retraining)
min_training_samples = 10000        # Minimum samples for training

# LSTM Configuration
lstm_sequence_length = 60           # 15 hours of 15-min candles
prediction_horizon = 1              # Next 15-min candle

# XGBoost Configuration
price_change_threshold = 0.005      # 0.5% for binary classification
n_estimators = 100                  # Reduced for faster training
early_stopping_rounds = 10          # Reduced for faster training

# Within-Window Data Split
train_ratio = 0.8                   # 80% training within each window
val_ratio = 0.2                     # 20% validation within each window
```

## ğŸ§  Why Walk-Forward Validation?

### ğŸ¯ Advantages Over Static Train/Test Split

| Method | Pros | Cons |
|--------|------|------|
| **Static Split** | Simple, fast | Overestimates performance, data leakage |
| **Walk-Forward** | Realistic, robust, no lookahead bias | Slower (multiple retrains) |

### ğŸ”„ Walk-Forward Benefits

1. **Realistic Performance**: Simulates actual trading conditions where models are retrained periodically
2. **No Lookahead Bias**: Each model only uses data available at that point in time
3. **Adaptive Learning**: Models adapt to changing market conditions over time
4. **Robust Evaluation**: Multiple out-of-sample tests provide better performance estimates
5. **Market Regime Detection**: Performance varies across different market conditions

### ğŸ“Š Interpretation of Results

- **Mean Performance**: Average across all windows
- **Standard Deviation**: Consistency of performance
- **Window-by-Window**: Shows performance evolution over time
- **Final Models**: Most recent models for actual trading

## ğŸ“ˆ Performance Expectations

### Typical Results

| Metric | Expected Range | Good Performance |
|--------|----------------|------------------|
| **LSTM MAE** | 0.005 - 0.015 | < 0.010 |
| **LSTM RMSE** | 0.010 - 0.025 | < 0.018 |
| **XGBoost Accuracy** | 0.55 - 0.75 | > 0.65 |
| **XGBoost AUC** | 0.60 - 0.85 | > 0.70 |

### Feature Importance

Typically, the most important features are:
1. **lstm_delta** (LSTM prediction)
2. **RSI** (momentum)
3. **MACD histogram** (trend)
4. **Price vs EMA** (trend position)
5. **Volume ratio** (market activity)

## ğŸš¨ Troubleshooting

### Common Issues

#### 1. pandas-ta Installation Error
```bash
# If pandas-ta installation fails, try:
pip install --upgrade pandas-ta

# Or install from source:
pip install git+https://github.com/twopirllc/pandas-ta
```

#### 2. Memory Issues
```python
# Reduce batch size in LSTM training
batch_size=16  # instead of 32

# Or reduce sequence length
lstm_sequence_length = 30  # instead of 60
```

#### 3. No Data Available
```bash
# Ensure data collector has run
python binance_data_collector.py

# Check data directory
ls data/
```

#### 4. TensorFlow GPU Issues
```python
# Force CPU usage if GPU causes problems
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
```

## ğŸ”„ Next Steps

After training completes:

1. **ğŸ“Š Analyze Results**: Review `results/training_summary.json`
2. **ğŸ” Feature Analysis**: Check feature importance files
3. **ğŸ§ª Backtesting**: Implement trading strategy using trained models
4. **ğŸ“ˆ Paper Trading**: Test with live data before real trading
5. **ğŸ”„ Retraining**: Set up periodic model updates

## ğŸ¯ Integration with Trading Bot

### Loading Trained Models

```python
import pickle
import tensorflow as tf

# Load LSTM model
lstm_model = tf.keras.models.load_model('models/lstm/btceur_lstm.h5')

# Load XGBoost model
with open('models/xgboost/btceur_xgboost.pkl', 'rb') as f:
    xgb_model = pickle.load(f)

# Load scaler
with open('models/scalers/btceur_scaler.pkl', 'rb') as f:
    scaler = pickle.load(f)
```

### Making Predictions

```python
# 1. Prepare LSTM input (last 60 candles)
lstm_input = prepare_lstm_sequence(recent_data)
lstm_input_scaled = scaler.transform(lstm_input)

# 2. Get LSTM prediction
lstm_delta = lstm_model.predict(lstm_input_scaled)

# 3. Prepare XGBoost features
features = create_technical_features(recent_data)
features['lstm_delta'] = lstm_delta[0]

# 4. Get final trading signal
trade_signal = xgb_model.predict_proba([features])[0][1]  # Probability of up move
```

## ğŸ“š References

- **LSTM**: [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- **XGBoost**: [XGBoost Documentation](https://xgboost.readthedocs.io/)
- **Technical Analysis**: [pandas-ta Documentation](https://github.com/twopirllc/pandas-ta)
- **Time Series**: [Scikit-learn Time Series](https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split)

---

**ğŸ‰ Happy Trading! Remember: Past performance doesn't guarantee future results. Always use proper risk management.**